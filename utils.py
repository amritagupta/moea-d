#!/usr/bin/python
"""
A single module containing helper functions for running multi-objective optimization
with the MOEA/D genetic algorithm.
"""

import numpy as np
from scipy.misc import comb
from sklearn.neighbors import NearestNeighbors


def generate_lambda_vectors(m, H=25):
	"""
	Generates lambda (weight) vectors for creating scalarized subroblems from a
	multi-objective optimization problem.

	Args:
		m: number of objectives in the multi-objective problem
		H: parameter determining what values elements in each lambda vector can take

	Returns:
		A list of lists, in which the ith sublist is the lambda vector characterizing
		the ith subproblem; and each lambda vector is of length m.
	"""
	N = int(comb(H+m-1, m-1)) # not convinced of this formula  # number of subproblems
	lambda_vectors = np.empty([N,m])
	possible_values = np.arange(0, 1+1/float(H), 1/float(H))

	for n in range(N):
		value_indices = np.random.choice(range(H+1), 2, replace=True)
		lambda_vectors[n,:] = possible_values[value_indices]

		# is this normalization step done in the paper?
		if sum(lambda_vectors[n,:]) > 0:
			lambda_vectors[n,:] = lambda_vectors[n,:]*float(1/sum(lambda_vectors[n,:]))
		else: # handle case where all weights are zero, leading to division by zero
			lambda_vectors[n,:] = np.ones([1,m])*float(1/m)

	return lambda_vectors

def get_lambda_neighborhoods(lambda_vectors, T=10):
	"""
	Computes the neighborhood of each subproblem defined by lambda, by finding the
	closest T lambda vectors.

	Args:
		lambda_vectors: weight vectors for all subproblems
		T: neighborhood size

	Returns:
		A list of lists, in which the ith sublist contains the indices of the subproblems
		in the neighborhood of the ith subproblem.
	"""
	N = len(lambda_vectors)
	B = np.empty([N,T+1], int)
	nbrs = NearestNeighbors(n_neighbors=T+1, algorithm='ball_tree', metric='euclidean').fit(lambda_vectors)
	distances, B = nbrs.kneighbors(lambda_vectors)
	B = B[:,1:T+1]

	return B

def g_te(xA,lambda_sub, ideal_z):
	"""
	Computes weighted Chebyshev metric for the difference between a solution and
	an ideal point.

	Args:
		xA: the solution to test against the ideal point
		lambda_sub: the lambda weight vector for the subproblem for which sol is a solution
		ideal_z: the ideal point

	Returns:
		The Chebyshev metric for the difference between the solution and the
		ideal point.
	"""
	gte = np.max(np.multiply(lambda_sub, np.abs(np.subtract(xA.objective_val, ideal_z) ) ) )

	return gte

def remove_newly_dominated_solutions(EP, offspring, objective_sense='min'):
	"""
	Remove from EP any solutions that are dominated by the offspring.
	:param EP: External popluation, a list of efficient solutions.
	:param offspring: A new solution generated by the genetic operators.
	:param objective_sense: Whether we 'min' or 'max' the objective.
	"""
	filtered_EP = []
	for es in EP:
		es_dominated_by_offspring = False
		if objective_sense == 'min':
			if np.greater_equal(es.objective_val, offspring.objective_val).all() and not np.equal(es.objective_val, offspring.objective_val).all():
				es_dominated_by_offspring = True
		elif objective_sense == 'max':
			if np.less_equal(es.objective_val, offspring.objective_val) and not np.equal(es.objective_val, offspring.objective_val):
				es_dominated_by_offspring = True
		else:
			raise ValueError('Objective sense should be either "min" or "max".')
		if not es_dominated_by_offspring:
			filtered_EP.append(es)

	return filtered_EP

def add_if_not_dominated(offspring, EP, objective_sense='min'):
	"""
	Add offspring to EP if it is not dominated by any solution in EP.
	:param offspring: A new solution generated by the genetic operators.
	:param EP: External popluation, a list of efficient solutions.
	:param objective_sense: Whether we 'min' or 'max' the objective.
	"""
	offspring_dominated_by_EP = False
	for es in EP:
		if objective_sense == 'min':
			if np.less_equal(es.objective_val, offspring.objective_val).all() and not np.equal(es.objective_val, offspring.objective_val).all():
				offspring_dominated_by_EP = True
		elif objective_sense == 'max':
			if np.greater_equal(es.objective_val, offspring.objective_val).all() and not np.equal(es.objective_val, offspring.objective_val).all():
				offspring_dominated_by_EP = True
		else:
			raise ValueError('Objective sense should be either "min" or "max".')
	if not offspring_dominated_by_EP:
		EP.append(offspring)

	return EP

def repair(solution, previous_solution, optimization_problem):

   fixed_solution = solution

   for i in len(solution.x):
      # binary
      if binary[i] == 1:
         if solution.x[i] =! previous_solution.x[i]:
            if solution.x[i] == 1:
               fixed_solution.x[i] = 0
            else:
               fixed_solution.x[i] = 1

         if fixed_solution.solution.check_feasible(optimization_problem):
            break

      elif binary[i] == 0:
         if solution.x[i] =! previous_solution.x[i]:
            fixed_solution.x[i] = (solution.x[i]+ fixed_solution.x[i])/2

          if fixed_solution.solution.check_feasible(optimization_problem):
            break

   return fixed_solution


